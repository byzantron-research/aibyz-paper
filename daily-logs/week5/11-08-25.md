### Date & Topic

- **Date:** 11th August, 2025 
- **Main Focus:** Focused on setting up the core scaffolding for the dataCollection module, including CLI, configuration, HTTP utilities, storage/provenance tracking, and base schemas for the paper *AI-Driven Validator Selection in PoS Blockchain Networks*

---

### 1. Search

- **What did you look for?**  
  - Standardized folder structures and naming conventions for blockchain data pipelines for data collections



- **Where did you search?**  
  - GitHub repositories with MARL and trust-based logic for understanding the best practises
  - ChatGPT and Perplexity for design pattern references
  

- **Useful sources found:** 
    - https://github.com/SPaDeS-Lab/mrl-pos-plus
    - https://ethereum.github.io/beacon-APIs/
    - https://youtu.be/rEdJyoPLSME?si=w61dqHJXIv9U0bIb
   

---

### 2. Investigate

- **What did you dig into?**  
  - Wrote the initial cli.py to orchestrate collect, curate, and features commands
  - Created config.yaml for endpoint/network and output settings
  - Added http.py with retry logic for robust API calls
  - Implemented storage.py for partitioned path generation and data writing
  - Created provenance.py for metadata logging per dataset write
  - Added schemas.py with canonical models for blocks, validators, attestations, and penalties
  - Added utils.py for timestamp and formatting helpers

- **Any patterns or surprises?**  
  - None
  
  

---

### 3. Reflect

- **What did you learn?**  
  - Having a well-structured data ingestion core early in the project ensures that future chain-specific collectors can plug in without refactoring the storage layer.

- **What’s next?**  
  - Implement curators to normalize raw → curated datasets and its code
  
  

---

### 4. Actions

- **Immediate actions:**  
  - Finalize schema versioning and manifest tracking for all datasets









Ask ChatGPT

  
